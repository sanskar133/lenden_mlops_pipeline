{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2dd95862-1a32-4a1d-85c3-97254dcbbcc5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import pyspark.sql.functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c9193008-f040-41b4-b09a-1a84976b3df2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "\n",
    "delete  from ispl_databricks.ff.model_logs_payload\n",
    "where   execution_duration_ms is  null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9275d15b-c2f7-44a8-8f5b-0c32fac1f1ca",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "select * from ispl_databricks.ff.model_logs_payload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d9840c3f-347f-4805-8338-2de30515a4a4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Databricks widgets (Job Parameters)\n",
    "dbutils.widgets.text(\"model_name\", \"ff_mw\", \"Model Name\")\n",
    "dbutils.widgets.text(\"mean_diff_threshold\", \"0.2\", \"Mean Diff Threshold (Fraction)\")\n",
    "dbutils.widgets.text(\"std_diff_threshold\", \"0.3\", \"Std Diff Threshold (Fraction)\")\n",
    "dbutils.widgets.text(\"pvalue_threshold\", \"0.05\", \"Chi-Square p-value Threshold\")\n",
    "\n",
    "# Read job parameter values\n",
    "model_name = dbutils.widgets.get(\"model_name\")\n",
    "mean_diff_threshold = float(dbutils.widgets.get(\"mean_diff_threshold\"))\n",
    "std_diff_threshold = float(dbutils.widgets.get(\"std_diff_threshold\"))\n",
    "pvalue_threshold = float(dbutils.widgets.get(\"pvalue_threshold\"))\n",
    "\n",
    "print(f\"üèÉ Running data drift detection for model: {model_name}\")\n",
    "print(f\"üîπ Mean threshold: {mean_diff_threshold}, Std threshold: {std_diff_threshold}, p-value: {pvalue_threshold}\")\n",
    "\n",
    "# Import dependencies\n",
    "from scipy.stats import chi2_contingency\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import date\n",
    "from pyspark.sql import functions as F\n",
    "import os\n",
    "import json\n",
    "# --- Load reference and current (inference) datasets ---\n",
    "\n",
    "curr_df = (\n",
    "    spark.read.table(\"ispl_databricks.ff.model_logs_payload\")\n",
    "    .filter(\"request_time >= current_date() - INTERVAL 7 DAYS\")\n",
    "    .filter(F.col(\"execution_duration_ms\").isNotNull())\n",
    "    .toPandas()\n",
    ")\n",
    "result_list = []\n",
    "\n",
    "for _, row in curr_df.iterrows():\n",
    "    data = row['request']\n",
    "    # If you want to access 'dataframe_records', you need to load the JSON string first\n",
    "    data_dict = json.loads(data)\n",
    "    result_list.append(data_dict['dataframe_records'][0])\n",
    "curr_df = pd.DataFrame(result_list)\n",
    "print(curr_df.columns)\n",
    "\n",
    "features_df = spark.table(\"ispl_databricks.model_logs.mw_final_feature_store\").toPandas().head(20)\n",
    "ref_df = features_df.drop([\"loan_id\"], axis=1)\n",
    "curr_df = curr_df.drop(columns = [\"loan_id\"])\n",
    "features = curr_df.columns.tolist()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# --- Define features to monitor ---\n",
    "cat_cols = ref_df.select_dtypes(include=[\"object\", \"category\"]).columns.tolist()\n",
    "num_cols = ref_df.select_dtypes(include=[\"int\", \"float\", \"number\"]).columns.tolist()\n",
    "\n",
    "results = []\n",
    "evaluation_date = date.today()\n",
    "\n",
    "# --- 1. Categorical Columns: Chi-Square Test ---\n",
    "for col in cat_cols:\n",
    "    try:\n",
    "        ref_counts = ref_df[col].value_counts()\n",
    "        cur_counts = curr_df[col].value_counts()\n",
    "\n",
    "        all_categories = set(ref_counts.index).union(set(cur_counts.index))\n",
    "        ref_aligned = [ref_counts.get(c, 0) for c in all_categories]\n",
    "        cur_aligned = [cur_counts.get(c, 0) for c in all_categories]\n",
    "\n",
    "        chi2, p, _, _ = chi2_contingency([ref_aligned, cur_aligned])\n",
    "        drift_status_cat = \"Drift\" if p < pvalue_threshold else \"Stable\"\n",
    "\n",
    "        results.append((\n",
    "            evaluation_date,\n",
    "            \"ff_mw\",\n",
    "            col,\n",
    "            \"categorical\",\n",
    "            \"chi_square\",\n",
    "            float(p),\n",
    "            drift_status_cat,\n",
    "            int(len(ref_df)),\n",
    "            int(len(curr_df)),\n",
    "            None\n",
    "        ))\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Skipped {col} due to error: {e}\")\n",
    "\n",
    "# --- 2. Numeric Columns: Mean & Std Deviation Comparison ---\n",
    "for col in num_cols:\n",
    "    try:\n",
    "        mean_ref, mean_cur = ref_df[col].mean(), curr_df[col].mean()\n",
    "        std_ref, std_cur = ref_df[col].std(), curr_df[col].std()\n",
    "\n",
    "        mean_diff = abs(mean_cur - mean_ref) / (abs(mean_ref) + 1e-6)\n",
    "        std_diff = abs(std_cur - std_ref) / (abs(std_ref) + 1e-6)\n",
    "\n",
    "        drift_status_num = \"Drift\" if (mean_diff > mean_diff_threshold or std_diff > std_diff_threshold) else \"Stable\"\n",
    "\n",
    "        results.append((\n",
    "            evaluation_date,\n",
    "            \"ff_mw\",\n",
    "            col,\n",
    "            \"numeric\",\n",
    "            \"mean_std\",\n",
    "            float(max(mean_diff, std_diff)),\n",
    "            drift_status_num,\n",
    "            int(len(ref_df)),\n",
    "            int(len(curr_df)),\n",
    "            None,\n",
    "            None\n",
    "        ))\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Skipped {col} due to error: {e}\")\n",
    "\n",
    "# -------------------------------------------------------------\n",
    "# ‚≠ê NEW: GLOBAL DRIFT STATUS (Do not modify per-column results)\n",
    "# -------------------------------------------------------------\n",
    "drift_count = sum(1 for r in results if r[6] == \"Drift\")  # index 6 = drift_status\n",
    "drift_status = \"Drift\" if drift_count >= 20 else \"Stable\"\n",
    "\n",
    "print(f\"üîî Per-feature drift count = {drift_count}\")\n",
    "print(f\"üåê Global drift status = {drift_status}\")\n",
    "# -------------------------------------------------------------\n",
    "\n",
    "# --- Create DataFrame & Write to Delta Table ---\n",
    "from pyspark.sql.types import (\n",
    "    StructType,\n",
    "    StructField,\n",
    "    StringType,\n",
    "    IntegerType,\n",
    "    FloatType,\n",
    "    DateType,\n",
    "    DoubleType,\n",
    "    TimestampType\n",
    "\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"evaluation_date\", DateType(), True),\n",
    "    StructField(\"model_name\", StringType(), True),\n",
    "    StructField(\"feature_name\", StringType(), True),\n",
    "    StructField(\"feature_type\", StringType(), True),\n",
    "    StructField(\"metric_used\", StringType(), True),\n",
    "    StructField(\"metric_value\", DoubleType(), True),\n",
    "    StructField(\"drift_status\", StringType(), True),\n",
    "    StructField(\"ref_sample_size\", IntegerType(), True),\n",
    "    StructField(\"cur_sample_size\", IntegerType(), True),\n",
    "    StructField(\"comment\", StringType(), True),\n",
    "    StructField(\"created_at\", TimestampType(), True),\n",
    "])\n",
    "\n",
    "if results:\n",
    "    drift_df = spark.createDataFrame(results, schema)\n",
    "    drift_df = drift_df.withColumn(\"created_at\", F.current_timestamp())\n",
    "    drift_df = drift_df.drop(\"global_drift_status\")\n",
    "    drift_df = drift_df.withColumn(\"metric_value\", drift_df[\"metric_value\"].cast(\"double\"))\n",
    "    drift_df.write.format(\"delta\").mode(\"append\").saveAsTable(\"ispl_databricks.model_logs.data_drift_log\")\n",
    "    display(drift_df.printSchema())\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No results to log ‚Äî check your reference and inference tables.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f9b58eb3-fcaf-4af0-8d56-968c30c3a1a8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "select * from ispl_databricks.model_logs.data_drift_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0301f995-3b57-4b45-a82f-e6baf1f22b06",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "select * from ispl_databricks.ff.model_logs_payload\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d6b009dc-f06c-4650-8fae-1a7461f9c305",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.jobs.taskValues.set(\"drift_flag\", drift_status)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 8330593345180858,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Data Drift - ff_mw",
   "widgets": {
    "mean_diff_threshold": {
     "currentValue": "0.2",
     "nuid": "aec44fd2-2910-401d-9291-e3eb3429e1a7",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "0.2",
      "label": "Mean Diff Threshold (Fraction)",
      "name": "mean_diff_threshold",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "0.2",
      "label": "Mean Diff Threshold (Fraction)",
      "name": "mean_diff_threshold",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "model_name": {
     "currentValue": "loan_approval_model",
     "nuid": "074a0a8e-223b-4a5a-adb2-4511810e007a",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "ff_mw",
      "label": "Model Name",
      "name": "model_name",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "ff_mw",
      "label": "Model Name",
      "name": "model_name",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "pvalue_threshold": {
     "currentValue": "0.05",
     "nuid": "e182f038-e2c9-42ca-b98a-4380135b19f0",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "0.05",
      "label": "Chi-Square p-value Threshold",
      "name": "pvalue_threshold",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "0.05",
      "label": "Chi-Square p-value Threshold",
      "name": "pvalue_threshold",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "std_diff_threshold": {
     "currentValue": "0.3",
     "nuid": "9e82497c-653c-4d24-9409-ad30eebf3706",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "0.3",
      "label": "Std Diff Threshold (Fraction)",
      "name": "std_diff_threshold",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "0.3",
      "label": "Std Diff Threshold (Fraction)",
      "name": "std_diff_threshold",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    }
   }
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
