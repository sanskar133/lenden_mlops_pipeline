{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f27eb690-6ec4-4ca9-b186-105a1cbf8aff",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install databricks-feature_engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a0e5d575-4330-435e-a551-85755a310df0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%restart_python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7a76aabb-8a6a-4280-a532-f24b3681d59b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import joblib\n",
    "from datetime import datetime, date\n",
    "from databricks.feature_engineering import FeatureEngineeringClient\n",
    "from databricks.feature_engineering import FeatureEngineeringClient, FeatureLookup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e8621c63-f284-43a9-93c4-b79b31677f96",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.widgets.text(\"training_csv\", \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b5ecf05f-10d9-4844-984f-2856e96ac2be",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "training_csv = dbutils.widgets.get(\"training_csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b189a3f9-28f7-432c-a610-c558856af75f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(training_csv)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1e90aac8-48c8-42c9-ae40-303589acf247",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b802f92d-daf9-4b1a-a210-cdf816110fcd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.columns.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fd925297-f7c6-438b-845b-610e97dc448e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "for col in df.select_dtypes(include='object').columns:\n",
    "    df[col] = df[col].astype('category')\n",
    "\n",
    "# Convert dates\n",
    "df['status_date'] = pd.to_datetime(df['lr_created_date'], errors='coerce')\n",
    "\n",
    "# Create month-year column\n",
    "df['month_year'] = df['status_date'].dt.strftime('%m-%Y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0e6a64e9-0795-4671-b641-a5a718fc766d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Assign type based on precise date ranges\n",
    "# For X_train use type='train' (2023-06 to 2024-09)\n",
    "# For X_test use type='test' (2024-10 full month) \n",
    "# For Live use type='live' (2024-11 full month)\n",
    "# Rest will be deleted\n",
    "# Assign type based on date ranges\n",
    "def assign_type(date):\n",
    "    if pd.Timestamp('2023-06-01') <= date < pd.Timestamp('2024-10-01'):\n",
    "        return 'train'\n",
    "    elif pd.Timestamp('2024-10-01') <= date < pd.Timestamp('2024-11-01'):\n",
    "        return 'test'\n",
    "    elif pd.Timestamp('2024-11-01') <= date < pd.Timestamp('2024-12-01'):\n",
    "        return 'live'\n",
    "    else:\n",
    "        return 'delete'\n",
    "\n",
    "df['type'] = df['status_date'].apply(assign_type)\n",
    "\n",
    "# Show counts\n",
    "print(df['type'].value_counts())\n",
    "\n",
    "# Filter and save datasets\n",
    "save_path = \"/Volumes/ispl_databricks/default/training/MW_Train/\"  # Databricks-compatible path\n",
    "\n",
    "# for dataset_type in ['train', 'test', 'live']:\n",
    "#     subset = df[df['type'] == dataset_type]\n",
    "#     subset.to_csv(f\"{save_path}{dataset_type}.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f9fa2e68-50e9-43ea-b8af-39bfb1c158dc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create monthwise distribution visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Create a monthwise breakdown\n",
    "df['year_month'] = df['status_date'].dt.to_period('M')\n",
    "\n",
    "# Get monthwise distribution by type\n",
    "monthly_dist = df.groupby(['year_month', 'type']).size().unstack(fill_value=0)\n",
    "\n",
    "# Create the visualization\n",
    "plt.figure(figsize=(15, 8))\n",
    "\n",
    "# Plot stacked bar chart\n",
    "monthly_dist.plot(kind='bar', stacked=True, figsize=(15, 8), \n",
    "                  color=['#1f77b4', '#ff7f0e', '#2ca02c'])  # Blue, Orange, Green\n",
    "\n",
    "plt.title('Monthwise Distribution of Train, Test, and Live Data', fontsize=16, fontweight='bold')\n",
    "plt.xlabel('Month-Year', fontsize=12)\n",
    "plt.ylabel('Number of Records', fontsize=12)\n",
    "plt.legend(title='Data Type', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "plt.xticks(rotation=45)\n",
    "plt.grid(axis='y', alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print detailed monthwise breakdown\n",
    "print(\"Monthwise Distribution:\")\n",
    "print(\"=\" * 50)\n",
    "print(monthly_dist)\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "\n",
    "# Print summary statistics\n",
    "print(\"\\nSummary Statistics:\")\n",
    "print(f\"Total records: {len(df):,}\")\n",
    "print(f\"Date range: {df['status_date'].min()} to {df['status_date'].max()}\")\n",
    "print(f\"Train data: {len(df[df['type'] == 'train']):,} records\")\n",
    "print(f\"Test data: {len(df[df['type'] == 'test']):,} records\") \n",
    "print(f\"Live data: {len(df[df['type'] == 'live']):,} records\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9ed321fd-6971-4bc0-b727-db9c1382a0a0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create a copy of the dataframe\n",
    "final = df.copy()\n",
    "\n",
    "\n",
    "# Convert '_is_' and '_exist' columns to category\n",
    "is_columns = [col for col in final.columns if '_is_' in col]\n",
    "exist_columns = [col for col in final.columns if col.endswith('_exist')]\n",
    "cat_columns = list(set(is_columns + exist_columns))  # combine lists\n",
    "\n",
    "# Convert all categorical columns at once\n",
    "final[cat_columns] = final[cat_columns].astype('category')\n",
    "print(f\"Number of categorical columns converted: {len(cat_columns)}\")\n",
    "\n",
    "\n",
    "# Replace common boolean/string values\n",
    "replace_dict = {\n",
    "    \"true\": 1, \"TRUE\": 1, True: 1,\n",
    "    \"false\": 0, \"FALSE\": 0, False: 0,\n",
    "    \"na\": np.nan,\n",
    "    \"error\": np.nan\n",
    "}\n",
    "\n",
    "final.replace(replace_dict, inplace=True)\n",
    "\n",
    "\n",
    "# Optional: Custom mapping for specific columns (example)\n",
    "# Example for a column like whatsapp_is_business\n",
    "# final['s3m_phone_data_primary_data_whatsapp_is_business'] = final['s3m_phone_data_primary_data_whatsapp_is_business'].map(\n",
    "#     lambda x: 1 if str(x) in ['1', '1.0'] else (0 if str(x) in ['0', '0.0'] else x)\n",
    "# )\n",
    "\n",
    "\n",
    "# Verify changes\n",
    "print(final[cat_columns].head())\n",
    "print(final[cat_columns].dtypes.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9b570482-ab68-4bd0-a6c0-7fb2bcc0b391",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def calculate_age(dob_str):\n",
    "    \"\"\"Calculate age from DD-MM-YYYY string. Returns -1 if invalid or <18.\"\"\"\n",
    "    if pd.isna(dob_str):\n",
    "        return -1\n",
    "    try:\n",
    "        born = datetime.strptime(str(dob_str), \"%d-%m-%Y\").date()\n",
    "        today = date.today()\n",
    "        age = today.year - born.year - ((today.month, today.day) < (born.month, born.day))\n",
    "        return age if age >= 18 else -1\n",
    "    except Exception:\n",
    "        return -1\n",
    "\n",
    "# Apply function vectorized\n",
    "final['Age'] = final['bue_dob'].apply(calculate_age)\n",
    "\n",
    "# Optional: check distribution\n",
    "print(final['Age'].value_counts(dropna=False).head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0b016994-b76b-453e-9664-3f56688859a1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Convert all object type columns to category\n",
    "object_columns = final.select_dtypes(include='object').columns\n",
    "final[object_columns] = final[object_columns].astype('category')\n",
    "\n",
    "# Identify categorical columns (both category + object, just in case)\n",
    "categorical_cols = final.select_dtypes(include=['category']).columns\n",
    "\n",
    "# Drop columns with > 50 unique categories (except 'required_loan_id')\n",
    "high_cardinality = [\n",
    "    col for col in categorical_cols \n",
    "    if col != 'required_loan_id' and final[col].nunique(dropna=True) > 50\n",
    "]\n",
    "\n",
    "final.drop(columns=high_cardinality, inplace=True)\n",
    "\n",
    "print(f\"Converted {len(object_columns)} object columns to category\")\n",
    "print(f\"Dropped {len(high_cardinality)} high-cardinality categorical columns\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "04051f6e-9514-4188-bcf8-eadfe0c5e377",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "ratio_features = {\n",
    "    \"bue_perc_no_of_open_loans\": (\"bue_no_of_open_loans\", \"bue_no_of_loans\"),\n",
    "    \"bue_perc_no_of_open_loans_lst_6months\": (\"bue_no_of_open_loans_lst_6months\", \"bue_no_of_loans\"),\n",
    "    \"bue_perc_no_of_cc_loans\": (\"bue_no_of_cc_loans\", \"bue_no_of_loans\"),\n",
    "    \"bue_perc_no_of_cc_open_loans\": (\"bue_no_of_cc_open_loans\", \"bue_no_of_open_loans\"),\n",
    "    \"bue_perc_no_of_auto_open_loans\": (\"bue_no_of_auto_open_loans\", \"bue_no_of_open_loans\"),\n",
    "    \"bue_perc_no_of_consumer_loans\": (\"bue_no_of_consumer_loans\", \"bue_no_of_loans\"),\n",
    "    \"bue_perc_no_of_consumer_open_loans\": (\"bue_no_of_consumer_open_loans\", \"bue_no_of_open_loans\"),\n",
    "    \"bue_perc_no_of_personal_loans\": (\"bue_no_of_personal_loans\", \"bue_no_of_loans\"),\n",
    "    \"bue_perc_no_of_gold_loans\": (\"bue_no_of_gold_loans\", \"bue_no_of_loans\"),\n",
    "}\n",
    "\n",
    "# Create ratios safely (avoid division by zero)\n",
    "for new_col, (num, denom) in ratio_features.items():\n",
    "    final[new_col] = np.where(\n",
    "        final[denom] > 0,\n",
    "        np.round(final[num] / final[denom], 2),\n",
    "        np.nan  # assign NaN if denominator is 0 or missing\n",
    "    )\n",
    "\n",
    "# Inspect a sample of created features along with target\n",
    "print(final[list(ratio_features.keys()) + ['bue_min_count_of_emi', 'target_30_dpd']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fb2af033-6196-4513-9e6f-37f3fe596f40",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Columns to drop\n",
    "drop_columns = [\n",
    "    'source','data_from','_merge','task_id','status','occupation',\n",
    "    'lr_created_date','bue_dob','disbursement_completion_date',\n",
    "    'month_year','Age','bue_min_count_of_emi'\n",
    "]\n",
    "\n",
    "# Inspect example column before dropping (optional)\n",
    "#print(final['s3e_email_data_linked_data_skype_creation_time'].value_counts(dropna=False).head())\n",
    "\n",
    "# Save dtypes for reference\n",
    "dtype_df = final.dtypes.reset_index()\n",
    "dtype_df.columns = ['column_name', 'dtype']\n",
    "dtype_df.to_csv(\"/Volumes/ispl_databricks/default/training/MW_Train/data_types.csv\", index=False)\n",
    "\n",
    "# Drop unwanted columns\n",
    "base_df1 = final.drop(columns=drop_columns, axis=1, errors='ignore')\n",
    "\n",
    "# Separate numeric and categorical\n",
    "df_num = base_df1.select_dtypes(include=['float64', 'int64', 'int32']).copy()\n",
    "df_cat = base_df1.select_dtypes(include=['object', 'boolean', 'category']).copy()\n",
    "\n",
    "# Fill missing numeric values with 0\n",
    "df_num = df_num.fillna(0)\n",
    "\n",
    "# Combine numeric + categorical back\n",
    "base_df2 = pd.concat([df_num, df_cat], axis=1)\n",
    "\n",
    "# Rename column safely\n",
    "base_df2 = base_df2.rename(columns={'required_loan_id': 'loan_id'})\n",
    "\n",
    "print(\"Final dataset shape:\", base_df2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e774a6ef-7a1a-4194-8646-966460e7ba0f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0b0d8652-203f-4039-8bde-e5960c7f6430",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# === Column Type Analysis ===\n",
    "print(\"=== Column Type Analysis ===\")\n",
    "print(f\"Total columns in base_df2: {len(base_df2.columns)}\")\n",
    "\n",
    "# Identify categorical columns\n",
    "categorical_columns = base_df2.select_dtypes(include=['object', 'category', 'boolean']).columns.tolist()\n",
    "print(f\"Number of categorical columns: {len(categorical_columns)}\")\n",
    "if categorical_columns:\n",
    "    print(\"Sample categorical columns:\", categorical_columns[:10], \"...\" if len(categorical_columns) > 10 else \"\")\n",
    "\n",
    "# Identify numerical columns\n",
    "numerical_columns = base_df2.select_dtypes(include=['int64', 'float64', 'int32', 'float32']).columns.tolist()\n",
    "print(f\"Number of numerical columns: {len(numerical_columns)}\")\n",
    "if numerical_columns:\n",
    "    print(\"Sample numerical columns:\", numerical_columns[:10], \"...\" if len(numerical_columns) > 10 else \"\")\n",
    "\n",
    "# Identify other/unsupported dtypes\n",
    "other_columns = base_df2.select_dtypes(\n",
    "    exclude=['object', 'category', 'boolean', 'int64', 'float64', 'int32', 'float32']\n",
    ").columns.tolist()\n",
    "print(f\"Number of other type columns: {len(other_columns)}\")\n",
    "if other_columns:\n",
    "    print(\"Other type columns:\", other_columns)\n",
    "\n",
    "# Summary by dtype\n",
    "print(\"\\n=== Data Type Summary ===\")\n",
    "print(base_df2.dtypes.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e164da14-d1f0-4060-a294-2f52e0932445",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0a30cbce-f46d-4bfc-ad7f-2e30c76a8958",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Calculate percentage of missing values per column\n",
    "null_percentages = base_df2.isnull().mean() * 100\n",
    "\n",
    "# Identify columns with >= 40% missing values\n",
    "columns_to_drop = null_percentages[null_percentages >= 40].index.tolist()\n",
    "\n",
    "# Drop those columns safely\n",
    "base_df2.drop(columns=columns_to_drop, inplace=True, errors='ignore')\n",
    "\n",
    "# Log info\n",
    "print(f\"Dropped {len(columns_to_drop)} columns with >=40% missing values:\")\n",
    "if columns_to_drop:\n",
    "    print(columns_to_drop[:10], \"...\" if len(columns_to_drop) > 10 else \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "236d06c0-1a8c-442e-8a7f-8e129c892259",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "label_encoders = {}\n",
    "\n",
    "for col in base_df2.columns:\n",
    "    dtype = base_df2[col].dtype\n",
    "\n",
    "    if col in [\"loan_id\",\"type\"]:\n",
    "        continue\n",
    "\n",
    "    if dtype.name in [\"category\", \"object\"]:\n",
    "        le = LabelEncoder()\n",
    "        base_df2[col] = le.fit_transform(\n",
    "            base_df2[col].astype(str)\n",
    "        )\n",
    "        label_encoders[col] = le"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3062197d-67ed-42db-af44-11dd9ccb52fe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "base_df2['type']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "95c36b27-12be-4f7c-932b-98b29bda94ab",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "base_df_spark = spark.createDataFrame(base_df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c6de2311-3949-4c31-9230-101e5775a4bb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "base_df_feature_store = base_df_spark.drop('target_30_dpd','type','target_source')\n",
    "base_4_feature_store = base_df_feature_store.drop_duplicates(['loan_id'])\n",
    "base_4_spark = base_df_spark.drop_duplicates(['loan_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "67c556aa-773d-4d1d-99a3-6bbb189b3d73",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "fe = FeatureEngineeringClient()\n",
    "fe.create_table(\n",
    "  name=\"ispl_databricks.model_logs.mw_feature_store_500\",\n",
    "  primary_keys=[\"loan_id\"],\n",
    "  df=base_4_feature_store,\n",
    "  description=\"Feature table for the bank\"\n",
    ")\n",
    "fe.write_table(\n",
    "    name=\"ispl_databricks.model_logs.mw_feature_store_500\",\n",
    "    df=base_4_feature_store,                 # Spark or pandas DataFrame\n",
    "    mode=\"merge\"           # works like upsert (recommended)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "32d768d1-2310-4e46-b8e8-c0156bc5ddd0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "base_4_spark.write \\\n",
    "  .format(\"delta\") \\\n",
    "  .mode(\"overwrite\") \\\n",
    "  .saveAsTable(\"ispl_databricks.model_logs.base_df_500features_updated\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "preprocessing_loggng_to_feature_store",
   "widgets": {
    "training_csv": {
     "currentValue": "/Volumes/ispl_databricks/default/training/MW_Train/merged_data_corrected.csv",
     "nuid": "7e9ec6db-14d2-4e78-9bfd-3563e213692a",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "",
      "label": null,
      "name": "training_csv",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "",
      "label": null,
      "name": "training_csv",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    }
   }
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
