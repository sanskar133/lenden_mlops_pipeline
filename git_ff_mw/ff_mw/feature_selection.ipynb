{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "482ec73a-2992-4ad9-a532-9c0d97b82ff6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "!pip install lightgbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "603ea481-f460-4c3a-a1b5-5550af6e19f8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "pip install hyperopt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7606faf0-fd8a-4aec-a390-110302b9c880",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install databricks-feature_engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c9648c3f-e015-4de6-96bf-8dbec51a008c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%restart_python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1ea39d06-d94e-4a66-96cc-9679eaa5c49b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import lightgbm as lgb\n",
    "import joblib\n",
    "from datetime import datetime, date\n",
    "import mlflow\n",
    "import mlflow.lightgbm\n",
    "from sklearn.metrics import classification_report,roc_auc_score,f1_score\n",
    "from mlflow.models.signature import infer_signature\n",
    "from hyperopt import fmin, tpe, hp, Trials, STATUS_OK\n",
    "from hyperopt.pyll.base import scope\n",
    "from databricks.feature_engineering import FeatureEngineeringClient\n",
    "from databricks.feature_engineering import FeatureEngineeringClient, FeatureLookup\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import json\n",
    "import requests\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "717e1742-0ae1-4ef4-99e7-85bb75db5c07",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Config flags and paths\n",
    "model_trainYN = 1\n",
    "data_version = \"base\"\n",
    "\n",
    "input_dir  = \"/Volumes/ispl_databricks/default/training/MW_Train/input_dir\"\n",
    "output_dir = \"/Volumes/ispl_databricks/default/training/MW_Train/OUTPUT_DIR_NEW\"\n",
    "model_dir  = \"/Volumes/ispl_databricks/default/training/MW_Train/model_dir\"\n",
    "\n",
    "\n",
    "# Create main directories if missing\n",
    "for directory in [output_dir, model_dir]:\n",
    "    os.makedirs(directory, exist_ok=True)\n",
    "\n",
    "\n",
    "# Version-specific directories\n",
    "# Choose data version (change here if needed)\n",
    "# data_version = \"data_v5_new/top_20\"\n",
    "input_dir_version  = os.path.join(input_dir, data_version)\n",
    "output_dir_version = os.path.join(output_dir, data_version)\n",
    "model_dir_version  = os.path.join(model_dir, data_version)\n",
    "\n",
    "# Create version-specific directories if missing\n",
    "for directory in [output_dir_version, model_dir_version]:\n",
    "    os.makedirs(directory, exist_ok=True)\n",
    "\n",
    "\n",
    "# Define model and feature file names\n",
    "model_file_name   = \"lgb_model.pickle\"\n",
    "feature_file_name = \"model_input_feature.pickle\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ccbcfc79-75bf-4724-802d-c285c15e426c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "base_df2 = spark.table(\"ispl_databricks.model_logs.base_df_500features_updated\").toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8e418929-6083-45b0-8a2d-18514840f836",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "model = pickle.load(open(os.path.join(model_dir_version, model_file_name), \"rb\"))\n",
    "model = joblib.load(os.path.join(model_dir_version, \"job_model.pkl\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "79f9332d-a767-4163-991d-47b2dbadba6e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(10, 8))\n",
    "ax = lgb.plot_importance(model, max_num_features=20, importance_type='gain')\n",
    "plt.title(\"Top 20 Feature Importances\")\n",
    "plt.show()\n",
    "\n",
    "# Optionally, log feature importance plot to MLflow\n",
    "mlflow.log_figure(ax.figure, \"feature_importance.png\")\n",
    "\n",
    "\n",
    "# Plot training evaluation metrics (logloss over iterations)\n",
    "plt.figure(figsize=(10, 6))\n",
    "ax = lgb.plot_metric(model)\n",
    "plt.title(\"Training Evaluation Metric (Binary Logloss)\")\n",
    "plt.show()\n",
    "\n",
    "# Optionally, log evaluation metric plot to MLflow\n",
    "mlflow.log_figure(ax.figure, \"training_evaluation.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "efa77ad5-2c97-4378-9a3a-9a861df1e64a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "feature_imp_df = pd.DataFrame({\n",
    "    'feature_name': model.feature_name_,\n",
    "    'importance': model.feature_importances_\n",
    "})\n",
    "\n",
    "# Sort features by importance descending\n",
    "feature_imp_df.sort_values(by=\"importance\", ascending=False, inplace=True)\n",
    "\n",
    "# Save all features with importance\n",
    "feature_imp_df.to_csv(os.path.join(output_dir_version, \"all_gain_features.csv\"), index=False)\n",
    "\n",
    "# Select top 50 features\n",
    "top_50_features = feature_imp_df.head(50)\n",
    "\n",
    "# Save top 50 features\n",
    "top_50_features.to_csv(os.path.join(output_dir_version, \"top50_gain_features.csv\"), index=False)\n",
    "\n",
    "# Export QC dataset with top 50 features\n",
    "top50_feature_names = top_50_features['feature_name'].tolist()\n",
    "base_df2[top50_feature_names].to_csv(os.path.join(output_dir_version, \"top_50_features_qc_data.csv\"), index=False)\n",
    "\n",
    "# Display top 50 features\n",
    "top_50_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "514ceb5a-2c1d-4687-a790-15c02f5f474d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "feature_to_store = top_50_features[\"feature_name\"].tolist()\n",
    "feature_to_store = feature_to_store + ['loan_id']\n",
    "available_features = [col for col in feature_to_store if col in base_df2.columns]\n",
    "feature_store_df = base_df2.loc[:, available_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "831dcb10-e8de-40ac-8959-36f606d071b2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "label_encoders = {}\n",
    "\n",
    "for col in feature_store_df.columns:\n",
    "    dtype = feature_store_df[col].dtype\n",
    "\n",
    "    if col == \"loan_id\":\n",
    "        continue\n",
    "\n",
    "    if dtype.name in [\"category\", \"object\"]:\n",
    "        le = LabelEncoder()\n",
    "        feature_store_df[col] = le.fit_transform(\n",
    "            feature_store_df[col].astype(str)\n",
    "        )\n",
    "        label_encoders[col] = le"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5fca8052-a58d-4314-9cab-e81c1ee35bcc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "feature_store_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6a0df321-4dce-4091-bec6-a7114468b256",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "feature_store_df  = spark.createDataFrame(feature_store_df)\n",
    "feature_store_df = feature_store_df.drop_duplicates(['loan_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "58171652-0578-4cbe-a58a-be2621c21103",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "fe = FeatureEngineeringClient()\n",
    "fe.create_table(\n",
    "  name=\"ispl_databricks.model_logs.mw_final_feature_store\",\n",
    "  primary_keys=[\"loan_id\"],\n",
    "  df=feature_store_df,\n",
    "  description=\"Feature table for the bank\"\n",
    ")\n",
    "fe.write_table(\n",
    "    name=\"ispl_databricks.model_logs.mw_final_feature_store\",\n",
    "    df=feature_store_df,                 # Spark or pandas DataFrame\n",
    "    mode=\"merge\"           # works like upsert (recommended)\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "feature_selection",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
