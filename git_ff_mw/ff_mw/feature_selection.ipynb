{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "482ec73a-2992-4ad9-a532-9c0d97b82ff6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "!pip install lightgbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "603ea481-f460-4c3a-a1b5-5550af6e19f8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "pip install hyperopt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7606faf0-fd8a-4aec-a390-110302b9c880",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install databricks-feature_engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c9648c3f-e015-4de6-96bf-8dbec51a008c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%restart_python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1ea39d06-d94e-4a66-96cc-9679eaa5c49b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import lightgbm as lgb\n",
    "import joblib\n",
    "from datetime import datetime, date\n",
    "import mlflow\n",
    "import mlflow.lightgbm\n",
    "from sklearn.metrics import classification_report,roc_auc_score,f1_score\n",
    "from mlflow.models.signature import infer_signature\n",
    "from hyperopt import fmin, tpe, hp, Trials, STATUS_OK\n",
    "from hyperopt.pyll.base import scope\n",
    "from databricks.feature_engineering import FeatureEngineeringClient\n",
    "from databricks.feature_engineering import FeatureEngineeringClient, FeatureLookup\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import json\n",
    "import requests\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4ef88387-a6aa-45ff-af18-650c896bae1b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "###### what this notebook is doing Model Loading\n",
    "\n",
    "##### Loads a trained LightGBM model\n",
    "\n",
    "##### Uses either pickle or joblib\n",
    "\n",
    "##### Feature Importance Extraction\n",
    "\n",
    "##### Pulls feature importance directly from the trained model\n",
    "\n",
    "##### Sorts features by gain/importance\n",
    "\n",
    "##### Feature Selection\n",
    "\n",
    "##### Saves:\n",
    "\n",
    "##### All features with importance\n",
    "\n",
    "##### Top 50 most important features\n",
    "\n",
    "##### Production Readiness\n",
    "\n",
    "##### Top 50 features can be:\n",
    "\n",
    "##### Registered in Feature Store\n",
    "\n",
    "##### Used for retraining\n",
    "\n",
    "##### Used for inference pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "717e1742-0ae1-4ef4-99e7-85bb75db5c07",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Config flags and paths\n",
    "model_trainYN = 1\n",
    "data_version = \"base\"\n",
    "\n",
    "input_dir  = \"/Volumes/ispl_databricks/default/training/MW_Train/input_dir\"\n",
    "output_dir = \"/Volumes/ispl_databricks/default/training/MW_Train/OUTPUT_DIR_NEW\"\n",
    "model_dir  = \"/Volumes/ispl_databricks/default/training/MW_Train/model_dir\"\n",
    "\n",
    "\n",
    "# Create main directories if missing\n",
    "for directory in [output_dir, model_dir]:\n",
    "    os.makedirs(directory, exist_ok=True)\n",
    "\n",
    "\n",
    "# Version-specific directories\n",
    "# Choose data version (change here if needed)\n",
    "# data_version = \"data_v5_new/top_20\"\n",
    "input_dir_version  = os.path.join(input_dir, data_version)\n",
    "output_dir_version = os.path.join(output_dir, data_version)\n",
    "model_dir_version  = os.path.join(model_dir, data_version)\n",
    "\n",
    "# Create version-specific directories if missing\n",
    "for directory in [output_dir_version, model_dir_version]:\n",
    "    os.makedirs(directory, exist_ok=True)\n",
    "\n",
    "\n",
    "# Define model and feature file names\n",
    "model_file_name   = \"lgb_model.pickle\"\n",
    "feature_file_name = \"model_input_feature.pickle\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ccbcfc79-75bf-4724-802d-c285c15e426c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "base_df2 = spark.table(\"ispl_databricks.model_logs.base_df_500features_updated\").toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "34fc37c5-eace-4423-9ec8-cc1c9771faa6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### loading model and features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8e418929-6083-45b0-8a2d-18514840f836",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# loading model\n",
    "model = pickle.load(open(os.path.join(model_dir_version, model_file_name), \"rb\"))\n",
    "model = joblib.load(os.path.join(model_dir_version, \"job_model.pkl\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "79f9332d-a767-4163-991d-47b2dbadba6e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(10, 8))\n",
    "ax = lgb.plot_importance(model, max_num_features=20, importance_type='gain')\n",
    "plt.title(\"Top 20 Feature Importances\")\n",
    "plt.show()\n",
    "\n",
    "# Optionally, log feature importance plot to MLflow\n",
    "mlflow.log_figure(ax.figure, \"feature_importance.png\")\n",
    "\n",
    "\n",
    "# Plot training evaluation metrics (logloss over iterations)\n",
    "plt.figure(figsize=(10, 6))\n",
    "ax = lgb.plot_metric(model)\n",
    "plt.title(\"Training Evaluation Metric (Binary Logloss)\")\n",
    "plt.show()\n",
    "\n",
    "# Optionally, log evaluation metric plot to MLflow\n",
    "mlflow.log_figure(ax.figure, \"training_evaluation.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "97b492ef-330b-4453-b66c-051a07a5e03a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### loading features with thier feature importance score sort them in desc order and  fetch top 50 features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "efa77ad5-2c97-4378-9a3a-9a861df1e64a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "feature_imp_df = pd.DataFrame({\n",
    "    'feature_name': model.feature_name_,\n",
    "    'importance': model.feature_importances_\n",
    "})\n",
    "\n",
    "# Sort features by importance descending\n",
    "feature_imp_df.sort_values(by=\"importance\", ascending=False, inplace=True)\n",
    "\n",
    "# Save all features with importance\n",
    "feature_imp_df.to_csv(os.path.join(output_dir_version, \"all_gain_features.csv\"), index=False)\n",
    "\n",
    "# Select top 50 features\n",
    "top_50_features = feature_imp_df.head(50)\n",
    "\n",
    "# Save top 50 features\n",
    "top_50_features.to_csv(os.path.join(output_dir_version, \"top50_gain_features.csv\"), index=False)\n",
    "\n",
    "# Export QC dataset with top 50 features\n",
    "top50_feature_names = top_50_features['feature_name'].tolist()\n",
    "base_df2[top50_feature_names].to_csv(os.path.join(output_dir_version, \"top_50_features_qc_data.csv\"), index=False)\n",
    "\n",
    "# Display top 50 features\n",
    "top_50_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3ddd2543-36ff-4770-ac37-f7ac38767489",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### create a spark dataframe with top 50 features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "514ceb5a-2c1d-4687-a790-15c02f5f474d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# --------------------------------------------\n",
    "# Extract only the feature names from the\n",
    "# top N features DataFrame\n",
    "# ----------------------------------------\n",
    "feature_to_store = top_50_features[\"feature_name\"].tolist()\n",
    "# Add the primary key (loan_id)\n",
    "# This is required for joins, feature store,\n",
    "# training, and inference consistency\n",
    "feature_to_store = feature_to_store + ['loan_id']\n",
    "# Select only the required columns from base data\n",
    "# This creates the final feature store DataFrame\n",
    "# containing:\n",
    "#   - Top N important features\n",
    "#   - Primary key (loan_id)\n",
    "available_features = [col for col in feature_to_store if col in base_df2.columns]\n",
    "feature_store_df = base_df2.loc[:, available_features]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1fa93fff-0b94-4d63-9135-b6320d2338d2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### creating feature store table for top 50 features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "831dcb10-e8de-40ac-8959-36f606d071b2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# just for precaution if any column is object type or category can be convert to int except loan_id\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "label_encoders = {}\n",
    "\n",
    "for col in feature_store_df.columns:\n",
    "    dtype = feature_store_df[col].dtype\n",
    "\n",
    "    if col == \"loan_id\":\n",
    "        continue\n",
    "\n",
    "    if dtype.name in [\"category\", \"object\"]:\n",
    "        le = LabelEncoder()\n",
    "        feature_store_df[col] = le.fit_transform(\n",
    "            feature_store_df[col].astype(str)\n",
    "        )\n",
    "        label_encoders[col] = le"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5fca8052-a58d-4314-9cab-e81c1ee35bcc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "feature_store_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6a0df321-4dce-4091-bec6-a7114468b256",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# creating spark dataframe as feature store only takes spark dataframe\n",
    "feature_store_df  = spark.createDataFrame(feature_store_df)\n",
    "# drop duplucates loan_id as loan_id is primary key\n",
    "feature_store_df = feature_store_df.drop_duplicates(['loan_id'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "65c8306f-b65d-409f-86a7-5a669e7e6570",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### finally storing features into feature store table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "58171652-0578-4cbe-a58a-be2621c21103",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# creation of feature store to store features that we will use to train top 50 features model\n",
    "fe = FeatureEngineeringClient()\n",
    "fe.create_table(\n",
    "  name=\"ispl_databricks.model_logs.mw_final_feature_store\",\n",
    "  primary_keys=[\"loan_id\"],\n",
    "  df=feature_store_df,\n",
    "  description=\"Feature table for the bank\"\n",
    ")\n",
    "fe.write_table(\n",
    "    name=\"ispl_databricks.model_logs.mw_final_feature_store\",\n",
    "    df=feature_store_df,                 # Spark or pandas DataFrame\n",
    "    mode=\"merge\"           # works like upsert (recommended)\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "feature_selection",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
