{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8777dfa7-86aa-4c27-b02c-456fe558df1c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "pip install lightgbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "00c71775-0390-46a7-a684-bec767b2b4fb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "pip install hyperopt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1883f5b4-f2f8-475d-a8c0-1ec24bfde7d6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install databricks-feature-engineering==0.13.0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ff42e885-047a-45f7-a41d-dba670fa4b8a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%restart_python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c2a48f7d-9540-4333-8171-075baf949726",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import lightgbm as lgb\n",
    "import pickle\n",
    "import joblib\n",
    "import os\n",
    "from datetime import datetime, date \n",
    "from sklearn.model_selection import train_test_split\n",
    "import mlflow\n",
    "import mlflow.lightgbm\n",
    "from sklearn.metrics import classification_report,roc_auc_score,f1_score\n",
    "import numpy as np\n",
    "from mlflow.models.signature import infer_signature\n",
    "from hyperopt import fmin, tpe, hp, Trials, STATUS_OK\n",
    "from hyperopt.pyll import scope\n",
    "import numpy as np\n",
    "from hyperopt import fmin, tpe, hp, Trials, STATUS_OK\n",
    "from hyperopt.pyll import scope\n",
    "from databricks.feature_engineering import FeatureEngineeringClient\n",
    "from databricks.feature_engineering import FeatureEngineeringClient, FeatureLookup\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import json\n",
    "from mlflow.tracking import MlflowClient\n",
    "import requests\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "466a146b-ed53-4aac-a1a7-b7f7cbe1e28a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**what this notebook is doing**\n",
    "Model Loading\n",
    "\n",
    "Loads a trained LightGBM model\n",
    "\n",
    "Uses either pickle or joblib\n",
    "\n",
    "Feature Importance Extraction\n",
    "\n",
    "Pulls feature importance directly from the trained model\n",
    "\n",
    "Sorts features by gain/importance\n",
    "\n",
    "Feature Selection\n",
    "\n",
    "Saves:\n",
    "\n",
    "All features with importance\n",
    "\n",
    "Top 50 most important features\n",
    "\n",
    "Production Readiness\n",
    "\n",
    "Top 50 features can be:\n",
    "\n",
    "Registered in Feature Store\n",
    "\n",
    "Used for retraining\n",
    "\n",
    "Used for inference pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e0ee63b2-21bc-4627-ac18-a08e9af507c5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "base_4 = spark.table('ispl_databricks.model_logs.bd_500_features_sample_training').toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "62c20cc6-3d46-4411-9d72-4a810d9f2860",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "model_trainYN = 1\n",
    "data_version = \"base\"\n",
    "\n",
    "input_dir = \"/Volumes/ispl_databricks/default/training/ff_bd/input_dir\"\n",
    "output_dir = \"/Volumes/ispl_databricks/default/training/ff_bd/output_dir_same_columns\"\n",
    "model_dir = \"/Volumes/ispl_databricks/default/training/ff_bd/model_dir\"\n",
    "\n",
    "# Set data version\n",
    "data_version = os.path.join(\"base\")   # change here\n",
    "# data_version = os.path.join(\"data_v5_new/top_20\")   # alternative version\n",
    "\n",
    "# Version-specific directories\n",
    "input_dir_version = os.path.join(input_dir, data_version)\n",
    "output_dir_version = os.path.join(output_dir, data_version)\n",
    "model_dir_version = os.path.join(model_dir, data_version)\n",
    "\n",
    "# Create directories if they do not exist\n",
    "for directory in [output_dir_version, model_dir_version]:\n",
    "    os.makedirs(directory, exist_ok=True)\n",
    "\n",
    "# Model and feature file names\n",
    "model_file_name = \"lgb_model.pickle\"\n",
    "feature_file_name = \"model_input_feature.pickle\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8173da4f-1c56-4d58-8a21-57f2bb0bcb0e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Load trained LightGBM model using pickle\n",
    "with open(\n",
    "    os.path.join(output_dir_version, model_file_name), \"rb\"\n",
    ") as f:\n",
    "    model = pickle.load(f)\n",
    "# Alternatively load model using joblib\n",
    "# (Preferred for sklearn / LightGBM models)\n",
    "model = joblib.load(\n",
    "    os.path.join(output_dir_version, \"job_model.pkl\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "020de05a-102d-43fb-bb15-fcd02a313105",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Extract feature importance from the model\n",
    "feature_imp_df = pd.DataFrame({'feature_name':model.feature_name_,'importance':model.feature_importances_})\n",
    "# Sort features by importance (highest first)\n",
    "feature_imp_df.sort_values(by=[\"importance\"],ascending=False,inplace=True)\n",
    "# Save full list of features with importance\n",
    "top_n_features = feature_imp_df.loc[:,'feature_name'].tolist()\n",
    "\n",
    "feature_imp_df.to_csv(os.path.join(output_dir_version,\"all_gain_feautures.csv\"))\n",
    "# Select top 50 most important features\n",
    "top_n_features = feature_imp_df.iloc[0:50]\n",
    "\n",
    "# --------------------------------------------\n",
    "# Save top 50 features for downstream use\n",
    "# (feature store / retraining / inference)\n",
    "# -------------------------------------------\n",
    "top_n_features.to_csv(os.path.join(output_dir_version,\"top50_gain_feautures.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "06eb4135-a685-410e-803d-887df5f3870e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# --------------------------------------------\n",
    "# Extract only the feature names from the\n",
    "# top N features DataFrame\n",
    "# ----------------------------------------\n",
    "feature_to_store = top_n_features[\"feature_name\"].tolist()\n",
    "# Add the primary key (loan_id)\n",
    "# This is required for joins, feature store,\n",
    "# training, and inference consistency\n",
    "feature_to_store = feature_to_store + ['loan_id']\n",
    "# Select only the required columns from base data\n",
    "# This creates the final feature store DataFrame\n",
    "# containing:\n",
    "#   - Top N important features\n",
    "#   - Primary key (loan_id)\n",
    "feature_store_df = base_4.loc[:,feature_to_store]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "01b25847-3b2c-4d7c-8273-26af72913e95",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# just for precaution if any column is object type or category can eb xonvert to int except loan_id\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "label_encoders = {}\n",
    "\n",
    "for col in feature_store_df.columns:\n",
    "    dtype = feature_store_df[col].dtype\n",
    "\n",
    "    if col == \"loan_id\":\n",
    "        continue\n",
    "\n",
    "    if dtype.name in [\"category\", \"object\"]:\n",
    "        le = LabelEncoder()\n",
    "        feature_store_df[col] = le.fit_transform(\n",
    "            feature_store_df[col].astype(str)\n",
    "        )\n",
    "        label_encoders[col] = le\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "817d7667-5eab-4c1a-8a58-33e470a85d02",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "feature_store_df  = spark.createDataFrame(feature_store_df)\n",
    "feature_store_df = feature_store_df.drop_duplicates(['loan_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d2eedf68-4675-43c0-8049-55c09ecbc868",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1767340676077}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(feature_store_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "95a2299d-ca59-4810-ac8f-f674234080fb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "feature_store_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c4cc7455-2656-44be-bb8a-593dfe76e881",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# creation of feature store to store features that we will use to train top 50 features model\n",
    "fe = FeatureEngineeringClient()\n",
    "fe.create_table(\n",
    "  name=\"ispl_databricks.model_logs.bd_final_feature_stores\",\n",
    "  primary_keys=[\"loan_id\"],\n",
    "  df=feature_store_df,\n",
    "  description=\"Feature table for the bank\"\n",
    ")\n",
    "fe.write_table(\n",
    "    name=\"ispl_databricks.model_logs.bd_final_feature_stores\",\n",
    "    df=feature_store_df,                 # Spark or pandas DataFrame\n",
    "    mode=\"merge\"           # works like upsert (recommended)\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "feature_selection",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
