{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "767efb3e-3f24-47e1-a5b5-e698eae778d6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "pip install lightgbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4f35f9da-7ea5-4302-afb8-5a895bd2ecaf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "pip install hyperopt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "782118d4-6d4d-4431-bac2-38988f9de3f4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install databricks-feature_engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2b6069ae-ea72-4490-b8da-582eb5104ec2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%restart_python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e0599d4a-d41e-4e65-a5be-b145aa8a8f43",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import lightgbm as lgb\n",
    "import joblib\n",
    "from datetime import datetime, date\n",
    "import mlflow\n",
    "import mlflow.lightgbm\n",
    "from sklearn.metrics import classification_report,roc_auc_score,f1_score\n",
    "from mlflow.models.signature import infer_signature\n",
    "from hyperopt import fmin, tpe, hp, Trials, STATUS_OK\n",
    "from hyperopt.pyll.base import scope\n",
    "from databricks.feature_engineering import FeatureEngineeringClient\n",
    "from databricks.feature_engineering import FeatureEngineeringClient, FeatureLookup\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import json\n",
    "import requests\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from pyspark.sql.functions import col\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d0ecf0f3-8ec7-461b-b17b-e04ad377f6e9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "This pipeline performs hyperparameter optimization for a LightGBM classifier using Hyperopt. For each trial, a model is trained on the training dataset, evaluated on a validation set, and tracked as a nested MLflow run. Hyperparameters and accuracy metrics are logged for experiment comparison, while validation log loss is computed and returned as the optimization objective. This design ensures systematic experimentation, reproducibility, and seamless model selection for production deployment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9a321a1e-d47a-4443-bdf9-fbb5ed951779",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Creating training and test dataset\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e635be67-d40d-44c8-af89-a76c75853d77",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Load base training data from Databricks table\n",
    "# This table contains loan_id and target label\n",
    "base_4 = spark.table('ispl_databricks.model_logs.bd_500_features_sample_training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f0e70a81-7db7-4d27-b293-2063899d6a1f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Extract only the primary key and target label\n",
    "# This will act as the label DataFrame\n",
    "spark_label =  base_4.select(col('loan_id'),col('target'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2e604f56-e1a2-47d0-937d-8854429ff59e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "fe = FeatureEngineeringClient()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "13542ac8-394d-4f17-a087-aa4f927479a8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create a Feature Store training set\n",
    "# This joins:\n",
    "#   - Labels (loan_id, target)\n",
    "#   - Features from Feature Store\n",
    "# Using loan_id as the lookup key\n",
    "training_set = fe.create_training_set(\n",
    "    df=spark_label,\n",
    "    feature_lookups=[\n",
    "        FeatureLookup(\n",
    "            table_name=\"ispl_databricks.model_logs.bd_final_feature_stores\",\n",
    "            lookup_key=\"loan_id\"\n",
    "        )\n",
    "    ],\n",
    "    label=\"target\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "60864132-64e2-49b7-afe7-ad9d1d394453",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# --------------------------------------------\n",
    "# Load the training set as a Spark DataFrame\n",
    "# Then convert it to Pandas for model training\n",
    "# --------------------------------------------\n",
    "train_pd = training_set.load_df().toPandas()\n",
    "# Remove rows with missing values\n",
    "# Ensures clean data for model training\n",
    "train_pd = train_pd.dropna()\n",
    "# Separate features (X) and target (y)\n",
    "# Remove:\n",
    "#   - loan_id (primary key)\n",
    "#   - target (label)\n",
    "train_x  = train_pd.drop(['loan_id','target'], axis=1)\n",
    "train_y = train_pd['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3257681b-542c-4a39-b0c1-f091a783aaa0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "train_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6eec4a54-69c0-4899-836c-afc957099059",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "train_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b9354796-6fea-494c-8b18-8e16f1488353",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "train_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "63ccbf66-0067-4baf-acbe-e2fb8418487a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "train_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a57b4f1d-2b96-4b56-a385-a7abee8f1c6e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(train_x, train_y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5dc79ec3-5576-4b8e-a03e-8d5c2996bf0c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7ce7eb3f-eef8-4fa2-a74f-cc5af4b916c6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "data_version = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ff30fb85-b714-43c0-8b05-3ad33a33d30b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### hyperopt experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "69cc384e-dfc0-4a94-a8bd-c67a157ebecb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "mlflow.set_experiment(\"/Workspace/Shared/ff_bd/LGBM_TopN_Features_Training\")\n",
    "if mlflow.active_run():\n",
    "    mlflow.end_run()\n",
    "\n",
    "# Train or load LGBMClassifier\n",
    "with mlflow.start_run(run_name=f\"LGBM_{data_version}\") as run:\n",
    "\n",
    "        # -----------------------------\n",
    "        # Hyperparameter tuning section\n",
    "        # -----------------------------\n",
    "    search_space = {\n",
    "            'num_leaves': scope.int(hp.quniform('num_leaves', 20, 150, 1)),\n",
    "            'max_depth': scope.int(hp.quniform('max_depth', 3, 15, 1)),\n",
    "            'learning_rate': hp.uniform('learning_rate', 0.01, 0.3),\n",
    "            'n_estimators': scope.int(hp.quniform('n_estimators', 100, 800, 50)),\n",
    "            'min_child_samples': scope.int(hp.quniform('min_child_samples', 10, 100, 5)),\n",
    "            'subsample': hp.uniform('subsample', 0.6, 1.0),\n",
    "            'colsample_bytree': hp.uniform('colsample_bytree', 0.6, 1.0)\n",
    "        }\n",
    "\n",
    "    def objective(params):\n",
    "            # Determine the current Hyperopt trial number\n",
    "            # trials.trials stores all completed trials so far\n",
    "        trial_id = len(trials.trials)\n",
    "            # Start an MLflow nested run for this trial\n",
    "            # Each Hyperopt trial is logged as a child run\n",
    "\n",
    "        with mlflow.start_run(run_name=f\"trial_{trial_id}_LGBM\",nested=True):\n",
    "              # ----------------------------------------------\n",
    "        # Initialize LightGBM classifier with:\n",
    "        # - Fixed random seed for reproducibility\n",
    "        # - Balanced class weights for imbalanced data\n",
    "        # - Hyperparameters suggested by Hyperopt\n",
    "        # ----------------------------------------------\n",
    "            model = lgb.LGBMClassifier(\n",
    "                random_state=42,\n",
    "                class_weight='balanced',\n",
    "                **params\n",
    "            )\n",
    "                  # Train the model using training data\n",
    "                  # Evaluate performance on validation data\n",
    "            model.fit(\n",
    "                X_train, y_train,\n",
    "                eval_set=[(X_test, y_test)],\n",
    "                eval_metric=\"binary_logloss\"\n",
    "            )\n",
    "            # Generate predictions on validation data\n",
    "            y_pred = model.predict(X_test)\n",
    "            accuracy = accuracy_score(y_test, y_pred)\n",
    "            mlflow.log_metric(\"accuracy\", accuracy)\n",
    "            mlflow.log_params(params)\n",
    "           \n",
    "        \n",
    "\n",
    "        \n",
    "\n",
    "       \n",
    "\n",
    "\n",
    "        val_pred_proba = model.predict_proba(X_test)[:, 1]\n",
    "        val_logloss = -((y_test * np.log(val_pred_proba) + (1 - y_test) * np.log(1 - val_pred_proba)).mean())\n",
    "\n",
    "        return {'loss': val_logloss, 'status': STATUS_OK}\n",
    "\n",
    "    trials = Trials()\n",
    "    best_params = fmin(\n",
    "            fn=objective,\n",
    "            space=search_space,\n",
    "            algo=tpe.suggest,\n",
    "            max_evals=30,\n",
    "            trials=trials,\n",
    "            rstate=np.random.default_rng(42)\n",
    "        )\n",
    "\n",
    "        # Convert integer-like floats back to ints\n",
    "    best_params = {\n",
    "            k: int(v) if isinstance(v, float) and v.is_integer() else v\n",
    "            for k, v in best_params.items()\n",
    "        }\n",
    "\n",
    "    mlflow.log_params(best_params)\n",
    "    mlflow.log_param(\"class_weight\", \"balanced\")\n",
    "    mlflow.log_param(\"random_state\", 42)\n",
    "\n",
    "        # -----------------------------\n",
    "        # Train final model with best params\n",
    "        # -----------------------------\n",
    "    model = lgb.LGBMClassifier(\n",
    "            random_state=42,\n",
    "            class_weight='balanced',\n",
    "            **best_params\n",
    "        )\n",
    "\n",
    "    model = model.fit(\n",
    "            X_train,\n",
    "            y_train,\n",
    "            eval_set=[(X_train, y_train), (X_test, y_test)],\n",
    "            eval_names=[\"train\", \"valid\"],\n",
    "            eval_metric=[\"binary_logloss\"]\n",
    "        )\n",
    "    acc = accuracy_score(y_test, model.predict(X_test))\n",
    "    mlflow.log_metric(\"test_accuracy\", acc)\n",
    "   \n",
    "  \n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "        # Infer model signature from training data and predictions\n",
    "    \n",
    "\n",
    "        # Save the trained model locally\n",
    "    joblib.dump(model,'/Workspace/Shared/ff_bd/model_artifacts/top50model.pkl')\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "595ce93e-d710-4f90-9ada-5371d46d8a81",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### saving model as well as metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d725e6b0-a5f7-4783-8d3a-a6c115a8428e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d12f48c2-9266-4e8e-911e-c03ad6613974",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# creating a metrics json which stores accuracy of our best model so thst we can store in model artifact folder and log in mlflow\n",
    "metrics_json =  {'accuracy':acc}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3d0fc550-7d15-4ac8-bbac-fd26e94d332e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# we are saving json in model_artifact folder it will help us to log accuracy in mlflow when we will log our best model\n",
    "with open('/Workspace/Shared/ff_bd/model_artifacts/model_metric.json','w') as f:\n",
    "    json.dump(metrics_json, f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "08530153-6a8c-4f88-a1e1-9f874375fc0b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# dumping model into model_artifacts\n",
    "joblib.dump(model,'/Workspace/Shared/ff_bd/model_artifacts/top50model.pkl')\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "top_50_training",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
